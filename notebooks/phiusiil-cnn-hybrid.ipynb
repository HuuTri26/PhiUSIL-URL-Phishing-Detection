{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!gdown --id 1F9XcsAkQ4fsN8IRVxRXfUwTtwqc5bd3N #X_test_embed\n!gdown --id 1XCnEDzTXGTXb4NrDYimZoBmyyf1w398w #X_train_embed\n!gdown --id 1jMYFF6px1ujxp7O13zMGdMLt-t7S7mjA #Y_test_embed\n!gdown --id 14_qieFo-rp9DDzoa6ESpMwwNx4VgsenY #Y_train_embed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T04:29:58.005269Z","iopub.execute_input":"2025-11-28T04:29:58.005932Z","iopub.status.idle":"2025-11-28T04:30:13.709643Z","shell.execute_reply.started":"2025-11-28T04:29:58.005896Z","shell.execute_reply":"2025-11-28T04:30:13.708805Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!gdown --id 1pJL2m0-7XwzkjgWLxA1gcrX-lPJPHvNi #X_test7\n!gdown --id 10uFQmiohMvXznxefaGU-YP0Kut6FHmtZ #X_train7\n!gdown --id 1TLGdPkJPKsXBtjlInIWcDAtWz3WWbCy6 #Y_test7\n!gdown --id 1XlvYDH_58T001jS89N7uUWboRC8Bw86o #Y_train7","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T04:30:55.770856Z","iopub.execute_input":"2025-11-28T04:30:55.771527Z","iopub.status.idle":"2025-11-28T04:31:10.760703Z","shell.execute_reply.started":"2025-11-28T04:30:55.771494Z","shell.execute_reply":"2025-11-28T04:31:10.759579Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!gdown --id 1C-YUtRF80X6SpRqDZcENeBLYE8wnUCzD # Char Tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T04:31:21.980859Z","iopub.execute_input":"2025-11-28T04:31:21.981593Z","iopub.status.idle":"2025-11-28T04:31:24.104672Z","shell.execute_reply.started":"2025-11-28T04:31:21.981554Z","shell.execute_reply":"2025-11-28T04:31:24.103796Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!gdown --id 1WuNvc0Hwi5OHnUhfuB9aVuoSvHgmg5OH #CNN-Hybrid model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T04:31:31.029178Z","iopub.execute_input":"2025-11-28T04:31:31.029937Z","iopub.status.idle":"2025-11-28T04:31:33.895446Z","shell.execute_reply.started":"2025-11-28T04:31:31.029904Z","shell.execute_reply":"2025-11-28T04:31:33.89464Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport joblib\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, Model\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom urllib.parse import urlparse\nimport re","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T04:31:44.370433Z","iopub.execute_input":"2025-11-28T04:31:44.371011Z","iopub.status.idle":"2025-11-28T04:31:44.375718Z","shell.execute_reply.started":"2025-11-28T04:31:44.370988Z","shell.execute_reply":"2025-11-28T04:31:44.374795Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_w2v = joblib.load(\"/kaggle/working/X_train_embed.pkl\")\nX_test_w2v = joblib.load(\"/kaggle/working/X_test_embed.pkl\")\nY_train_w2v = joblib.load(\"/kaggle/working/Y_train_embed.pkl\")\nY_test_w2v = joblib.load(\"/kaggle/working/Y_test_embed.pkl\")\nX_train_c2v = joblib.load(\"/kaggle/working/X_train7.pkl\")\nX_test_c2v = joblib.load(\"/kaggle/working/X_test7.pkl\")\nY_train_c2v = joblib.load(\"/kaggle/working/Y_train7.pkl\")\nY_test_c2v = joblib.load(\"/kaggle/working/Y_test7.pkl\")\n\ntokenizer = joblib.load(\"/kaggle/working/PhiUSIIL_Char_Tokenizer.joblib\")\nvocab_size = len(tokenizer.word_index) + 1\nmodel = load_model(\"/kaggle/working/PhiUSIIL_CNN_Hybrid.keras\")\n\nprint(f\"\\nâœ… Data loaded successfully!\")\nprint(f\"   ðŸ“Š Word2Vec embeddings: Train={X_train_w2v.shape}, Test={X_test_w2v.shape}\")\nprint(f\"   ðŸ“Š Char sequences: Train={X_train_c2v.shape}, Test={X_test_c2v.shape}\")\nprint(f\"   ðŸ“Š Labels: Train={Y_train_w2v.shape}, Test={Y_test_w2v.shape}\")\nprint(f\"   ðŸ“š Char vocabulary size: {vocab_size}\")\nprint(f\"   ðŸ“Š Class distribution: Benign={np.sum(Y_train_w2v == 1):,}, Malicious={np.sum(Y_train_w2v == 0):,}\")\n\nclasses = np.unique(Y_train_w2v)\nweights = compute_class_weight(class_weight='balanced', classes=classes, y=Y_train_w2v)\nclass_weights = dict(zip(classes, weights))\nprint(f\"   ðŸ“Š Class weight: {class_weights}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T04:31:48.401893Z","iopub.execute_input":"2025-11-28T04:31:48.402209Z","iopub.status.idle":"2025-11-28T04:31:52.51401Z","shell.execute_reply.started":"2025-11-28T04:31:48.40216Z","shell.execute_reply":"2025-11-28T04:31:52.513289Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def PhiUSIIL_CNN_Hybrid(word_embedding_dim=50, char_embedding_dim=32, char_max_len=200, vocab_size=106,\n        num_filters=256, filter_sizes=[3, 4, 5, 6], dropout_rate=0.5, fc_units=[512, 256, 128]):\n    # ========================================== Character-Level =======================================================\n    char_input = layers.Input(shape=(char_max_len,), name='char_input')\n    # Char Embedding Layer\n    char_embedding = layers.Embedding(input_dim=vocab_size, output_dim=char_embedding_dim,\n                                     input_length=char_max_len, name='char_embedding',\n                                     embeddings_regularizer=keras.regularizers.l2(1e-6))(char_input)\n    # Parallel Convolutional Filters\n    char_conv_blocks = []\n    for filter_size in filter_sizes:\n        conv = layers.Conv1D(filters=num_filters, kernel_size=filter_size, activation='relu',\n                             padding='same', kernel_regularizer=keras.regularizers.l2(1e-6),\n                             name=f'char_conv_{filter_size}')(char_embedding)\n        # Batch Normalization\n        conv = layers.BatchNormalization(name=f'char_bn_{filter_size}')(conv)\n        # Global Max Pooling\n        pool = layers.GlobalMaxPooling1D(name=f'char_pool_{filter_size}')(conv)\n        char_conv_blocks.append(pool)\n    # Concatenate all pooled features\n    char_features = layers.Concatenate(name='char_concat')(char_conv_blocks)\n    char_features = layers.Dropout(dropout_rate, name='char_dropout')(char_features)\n    char_features = layers.Dense(fc_units[0], activation='relu',\n                                 kernel_regularizer=keras.regularizers.l2(1e-6), name='char_fc')(char_features)\n    char_features = layers.BatchNormalization(name='char_fc_bn')(char_features)\n    # ========================================== Word-Level ==============================================================\n    word_input = layers.Input(shape=(word_embedding_dim,), name='word_input')\n    # Dense layers\n    word_features = layers.Dense(512, activation='relu',\n                                 kernel_regularizer=keras.regularizers.l2(1e-6), name='word_fc1')(word_input)\n    word_features = layers.BatchNormalization(name='word_bn1')(word_features)\n    word_features = layers.Dropout(dropout_rate, name='word_dropout1')(word_features)\n\n    word_features = layers.Dense(fc_units[0], activation='relu',\n                                 kernel_regularizer=keras.regularizers.l2(1e-6), name='word_fc2')(word_features)\n    word_features = layers.BatchNormalization(name='word_bn2')(word_features)\n    word_features = layers.Dropout(dropout_rate, name='word_dropout2')(word_features)\n    # ======================================== Concatenate Char & Word ===================================================\n    combined = layers.Concatenate(name='combined_features')([char_features, word_features])\n    # ========================================== Classification Head =====================================================\n    x = combined\n    for i, units in enumerate (fc_units[1:], 1):\n        x = layers.Dense(units, activation='relu', kernel_regularizer=keras.regularizers.l2(1e-2), \n                        name=f'fc_{i}')(x)\n        x = layers.BatchNormalization(name=f'fc_bn_{i}')(x)\n        x = layers.Dropout(dropout_rate, name=f'dropout_{i}')(x)\n    # ============================================ Output layer ==========================================================\n    output = layers.Dense(1, activation='sigmoid', name='output')(x)\n    # ============================================= Build Model ==========================================================\n    model = Model(inputs=[char_input, word_input], outputs=output, name='PhiUSIIL_CNN_Hybrid')\n\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T04:15:11.311548Z","iopub.execute_input":"2025-11-10T04:15:11.311974Z","iopub.status.idle":"2025-11-10T04:15:11.327295Z","shell.execute_reply.started":"2025-11-10T04:15:11.311941Z","shell.execute_reply":"2025-11-10T04:15:11.326228Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nðŸ—ï¸ Building PhiUSIIL_CNN_Hybrid model...\")\nmodel = PhiUSIIL_CNN_Hybrid(\n    word_embedding_dim=50, char_embedding_dim=32, char_max_len=200, vocab_size=vocab_size,\n    num_filters=256, filter_sizes=[3, 4, 5, 6], dropout_rate=0.5, fc_units=[512, 256, 128] )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T04:15:14.137434Z","iopub.execute_input":"2025-11-10T04:15:14.138104Z","iopub.status.idle":"2025-11-10T04:15:14.267704Z","shell.execute_reply.started":"2025-11-10T04:15:14.13808Z","shell.execute_reply":"2025-11-10T04:15:14.266971Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.compile(\n    optimizer=Adam(learning_rate=0.001, clipnorm=1.0),\n    loss='binary_crossentropy',\n    metrics=[\n        'accuracy',\n        keras.metrics.AUC(name='auc'),\n        keras.metrics.Precision(name='precision'),\n        keras.metrics.Recall(name='recall')\n    ]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T04:15:28.057568Z","iopub.execute_input":"2025-11-10T04:15:28.058369Z","iopub.status.idle":"2025-11-10T04:15:28.085716Z","shell.execute_reply.started":"2025-11-10T04:15:28.058331Z","shell.execute_reply":"2025-11-10T04:15:28.085133Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nðŸ“Š Model Architecture:\")\nmodel.summary()\ntotal_params = model.count_params()\nprint(f\"\\nðŸ’¡ Total Parameters: {total_params:,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T04:15:31.398252Z","iopub.execute_input":"2025-11-10T04:15:31.398836Z","iopub.status.idle":"2025-11-10T04:15:31.441081Z","shell.execute_reply.started":"2025-11-10T04:15:31.398813Z","shell.execute_reply":"2025-11-10T04:15:31.440488Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"callbacks = [\n    ModelCheckpoint(\n        filepath='/kaggle/working/PhiUSIIL_CNN_Hybrid_Checkpoints.keras',\n        save_best_only=True,\n        verbose=1\n    ),\n    EarlyStopping(\n        monitor='val_loss',\n        mode='max',\n        patience=7,\n        verbose=1,\n        restore_best_weights=True\n    ),\n    ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.5,\n        patience=3,\n        verbose=1,\n        min_lr=1e-7\n    )\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T04:16:13.209762Z","iopub.execute_input":"2025-11-10T04:16:13.210007Z","iopub.status.idle":"2025-11-10T04:16:13.214509Z","shell.execute_reply.started":"2025-11-10T04:16:13.209991Z","shell.execute_reply":"2025-11-10T04:16:13.213817Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n\" + \"=\" * 80)\nprint(\"ðŸš€ TRAINING PhiUSIIL-CNN-Hybrid Model\")\nprint(\"=\" * 80)\n\nresult = model.fit(\n    [X_train_c2v, X_train_w2v],\n    Y_train_w2v,\n    batch_size=512,\n    epochs=100,\n    validation_split=0.15,\n    callbacks=callbacks,\n    class_weight=class_weights,\n    verbose=1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T04:16:53.450757Z","iopub.execute_input":"2025-11-10T04:16:53.451583Z","iopub.status.idle":"2025-11-10T04:22:02.35227Z","shell.execute_reply.started":"2025-11-10T04:16:53.451557Z","shell.execute_reply":"2025-11-10T04:22:02.351365Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n\" + \"=\" * 80)\nprint(\"ðŸ“ˆ EVALUATING MODEL ON TEST SET\")\nprint(\"=\" * 80)\n\ntest_results = model.evaluate(\n    [X_test_c2v, X_test_w2v],\n    Y_test_w2v,\n    batch_size=512,\n    verbose=1\n)\n\nprint(f\"\\nâœ… Test Results:\")\nprint(f\"   - Loss: {test_results[0]:.4f}\")\nprint(f\"   - Accuracy: {test_results[1]:.4f}\")\nprint(f\"   - AUC: {test_results[2]:.4f}\")\nprint(f\"   - Precision: {test_results[3]:.4f}\")\nprint(f\"   - Recall: {test_results[4]:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T04:26:42.656856Z","iopub.execute_input":"2025-11-10T04:26:42.657293Z","iopub.status.idle":"2025-11-10T04:26:47.372396Z","shell.execute_reply.started":"2025-11-10T04:26:42.657262Z","shell.execute_reply":"2025-11-10T04:26:47.371791Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nðŸ”® Generating predictions...\")\ny_pred_proba = model.predict([X_test_c2v, X_test_w2v], batch_size=512, verbose=1)\ny_pred = (y_pred_proba > 0.5).astype(int).flatten()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T04:31:32.589165Z","iopub.execute_input":"2025-11-10T04:31:32.590008Z","iopub.status.idle":"2025-11-10T04:31:37.07305Z","shell.execute_reply.started":"2025-11-10T04:31:32.589981Z","shell.execute_reply":"2025-11-10T04:31:37.072414Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"ðŸ“Š DETAILED PERFORMANCE METRICS\")\nprint(\"=\" * 80)\n\nprint(\"\\nðŸ“‹ Classification Report:\")\nprint(classification_report(\n    Y_test_w2v, \n    y_pred, \n    target_names=['Phishing (0)', 'Benign (1)'],\n    digits=4\n))\n\nprint(\"\\nðŸ”¢ Confusion Matrix:\")\ncm = confusion_matrix(Y_test_w2v, y_pred)\nprint(cm)\nprint(f\"\\nTrue Negatives (TN): {cm[0,0]:,}\")\nprint(f\"False Positives (FP): {cm[0,1]:,}\")\nprint(f\"False Negatives (FN): {cm[1,0]:,}\")\nprint(f\"True Positives (TP): {cm[1,1]:,}\")\n\n# Calculate additional metrics\nfpr_rate = cm[0,1] / (cm[0,0] + cm[0,1])\nfnr_rate = cm[1,0] / (cm[1,0] + cm[1,1])\nprint(f\"\\nFalse Positive Rate: {fpr_rate:.4f}\")\nprint(f\"False Negative Rate: {fnr_rate:.4f}\")\n\n# ROC-AUC\nroc_auc = roc_auc_score(Y_test_w2v, y_pred_proba)\nprint(f\"\\nðŸŽ¯ ROC-AUC Score: {roc_auc:.4f}\")\n\n# ==================== VISUALIZATIONS ====================\nprint(\"\\nðŸ“Š Creating visualizations...\")\n\nfig = plt.figure(figsize=(20, 12))\ngs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n\n# 1. Training History - Loss\nax1 = fig.add_subplot(gs[0, 0])\nax1.plot(result.history['loss'], label='Train Loss', linewidth=2)\nax1.plot(result.history['val_loss'], label='Val Loss', linewidth=2)\nax1.set_title('Model Loss', fontsize=14, fontweight='bold')\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Loss')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# 2. Training History - Accuracy\nax2 = fig.add_subplot(gs[0, 1])\nax2.plot(result.history['accuracy'], label='Train Accuracy', linewidth=2)\nax2.plot(result.history['val_accuracy'], label='Val Accuracy', linewidth=2)\nax2.set_title('Model Accuracy', fontsize=14, fontweight='bold')\nax2.set_xlabel('Epoch')\nax2.set_ylabel('Accuracy')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\n# 3. Training History - AUC\nax3 = fig.add_subplot(gs[0, 2])\nax3.plot(result.history['auc'], label='Train AUC', linewidth=2)\nax3.plot(result.history['val_auc'], label='Val AUC', linewidth=2)\nax3.set_title('Model AUC', fontsize=14, fontweight='bold')\nax3.set_xlabel('Epoch')\nax3.set_ylabel('AUC')\nax3.legend()\nax3.grid(True, alpha=0.3)\n\n# 4. Confusion Matrix\nax4 = fig.add_subplot(gs[1, 0])\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax4, \n            xticklabels=['Phishing', 'Benign'],\n            yticklabels=['Phishing', 'Benign'])\nax4.set_title('Confusion Matrix', fontsize=14, fontweight='bold')\nax4.set_ylabel('True Label')\nax4.set_xlabel('Predicted Label')\n\n# 5. ROC Curve\nax5 = fig.add_subplot(gs[1, 1])\nfpr, tpr, thresholds = roc_curve(Y_test_w2v, y_pred_proba)\nax5.plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC = {roc_auc:.4f})')\nax5.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\nax5.set_title('ROC Curve', fontsize=14, fontweight='bold')\nax5.set_xlabel('False Positive Rate')\nax5.set_ylabel('True Positive Rate')\nax5.legend()\nax5.grid(True, alpha=0.3)\n\n# 6. Prediction Distribution\nax6 = fig.add_subplot(gs[1, 2])\nax6.hist(y_pred_proba[Y_test_w2v == 0], bins=50, alpha=0.6, label='Malicious', color='red')\nax6.hist(y_pred_proba[Y_test_w2v == 1], bins=50, alpha=0.6, label='Benign', color='green')\nax6.axvline(0.5, color='black', linestyle='--', linewidth=2, label='Threshold')\nax6.set_title('Prediction Distribution', fontsize=14, fontweight='bold')\nax6.set_xlabel('Predicted Probability')\nax6.set_ylabel('Frequency')\nax6.legend()\nax6.grid(True, alpha=0.3)\n\n# 7. Precision-Recall\nax7 = fig.add_subplot(gs[2, 0])\nax7.plot(result.history['precision'], label='Train Precision', linewidth=2)\nax7.plot(result.history['val_precision'], label='Val Precision', linewidth=2)\nax7.set_title('Precision', fontsize=14, fontweight='bold')\nax7.set_xlabel('Epoch')\nax7.set_ylabel('Precision')\nax7.legend()\nax7.grid(True, alpha=0.3)\n\n# 8. Recall\nax8 = fig.add_subplot(gs[2, 1])\nax8.plot(result.history['recall'], label='Train Recall', linewidth=2)\nax8.plot(result.history['val_recall'], label='Val Recall', linewidth=2)\nax8.set_title('Recall', fontsize=14, fontweight='bold')\nax8.set_xlabel('Epoch')\nax8.set_ylabel('Recall')\nax8.legend()\nax8.grid(True, alpha=0.3)\n\n# 9. Metrics Summary Table\nax9 = fig.add_subplot(gs[2, 2])\nax9.axis('tight')\nax9.axis('off')\n\nmetrics_data = [\n    ['Metric', 'Value'],\n    ['Accuracy', f'{test_results[1]:.4f}'],\n    ['AUC', f'{test_results[2]:.4f}'],\n    ['Precision', f'{test_results[3]:.4f}'],\n    ['Recall', f'{test_results[4]:.4f}'],\n    ['F1-Score', f'{2 * test_results[3] * test_results[4] / (test_results[3] + test_results[4]):.4f}'],\n    ['FPR', f'{fpr_rate:.4f}'],\n    ['FNR', f'{fnr_rate:.4f}']\n]\n\ntable = ax9.table(cellText=metrics_data, cellLoc='center', loc='center',\n                  colWidths=[0.4, 0.3])\ntable.auto_set_font_size(False)\ntable.set_fontsize(11)\ntable.scale(1, 2.5)\n\nfor i in range(len(metrics_data)):\n    if i == 0:\n        table[(i, 0)].set_facecolor('#4CAF50')\n        table[(i, 1)].set_facecolor('#4CAF50')\n        table[(i, 0)].set_text_props(weight='bold', color='white')\n        table[(i, 1)].set_text_props(weight='bold', color='white')\n    else:\n        table[(i, 0)].set_facecolor('#E8F5E9')\n        table[(i, 1)].set_facecolor('#E8F5E9')\n\nplt.suptitle('PhiUSIIL_CNN_Hybrid - Complete Performance Analysis', \n             fontsize=16, fontweight='bold', y=0.995)\nplt.savefig('/kaggle/working/PhiUSIIL_CNN_Hybrid_results.png', \n            dpi=300, bbox_inches='tight')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T04:37:41.54083Z","iopub.execute_input":"2025-11-10T04:37:41.54113Z","iopub.status.idle":"2025-11-10T04:37:46.147781Z","shell.execute_reply.started":"2025-11-10T04:37:41.541106Z","shell.execute_reply":"2025-11-10T04:37:46.146954Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rng = np.random.RandomState(42)\n\ndef stratified_sample_indices(y, n_per_class, random_state=None):\n    \"\"\"Return indices with exactly n_per_class for each unique class in y.\n       Throws if class sizes < n_per_class.\n    \"\"\"\n    random_state = random_state or np.random\n    classes = np.unique(y)\n    idxs = []\n    for c in classes:\n        pos = np.where(y == c)[0]\n        if len(pos) < n_per_class:\n            raise ValueError(f\"Not enough samples for class {c}: have {len(pos)}, need {n_per_class}\")\n        chosen = random_state.choice(pos, size=n_per_class, replace=False)\n        idxs.append(chosen)\n    return np.concatenate(idxs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T04:32:04.642198Z","iopub.execute_input":"2025-11-28T04:32:04.642524Z","iopub.status.idle":"2025-11-28T04:32:04.648746Z","shell.execute_reply.started":"2025-11-28T04:32:04.642499Z","shell.execute_reply":"2025-11-28T04:32:04.648081Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bg_per_class = 1000   # 2000 total\ntest_per_class = 5000 # 10000 total\nY = np.asarray(Y_train_w2v)\n\nidx_bg = stratified_sample_indices(Y, n_per_class=bg_per_class, random_state=rng)\n# remove used indices from pool for test to avoid overlap\nremaining_idx = np.setdiff1d(np.arange(len(Y)), idx_bg)\nY_remaining = Y[remaining_idx]\n\n# sample test from remaining in balanced way\ndef stratified_from_pool(pool_idx, pool_y, n_per_class, random_state=None):\n    random_state = random_state or np.random\n    classes = np.unique(pool_y)\n    idxs = []\n    for c in classes:\n        pos_in_pool = pool_idx[pool_y == c]\n        if len(pos_in_pool) < n_per_class:\n            raise ValueError(f\"Not enough samples in pool for class {c}: have {len(pos_in_pool)}, need {n_per_class}\")\n        chosen = random_state.choice(pos_in_pool, size=n_per_class, replace=False)\n        idxs.append(chosen)\n    return np.concatenate(idxs)\n\nidx_test = stratified_from_pool(remaining_idx, Y_remaining, n_per_class=test_per_class, random_state=rng)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T04:37:23.175259Z","iopub.execute_input":"2025-11-28T04:37:23.175576Z","iopub.status.idle":"2025-11-28T04:37:23.22596Z","shell.execute_reply.started":"2025-11-28T04:37:23.17555Z","shell.execute_reply":"2025-11-28T04:37:23.225342Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_bg_w2v = X_train_w2v[idx_bg]\nX_bg_c2v = X_train_c2v[idx_bg]\nY_bg = Y[idx_bg]\n\nX_test_w2v = X_train_w2v[idx_test]\nX_test_c2v = X_train_c2v[idx_test]\nY_test = Y[idx_test]\n\nprint(\"Background shapes:\", X_bg_c2v.shape, X_bg_w2v.shape, Y_bg.shape)\nprint(\"Test shapes:\", X_test_c2v.shape, X_test_w2v.shape, Y_test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T04:40:12.284434Z","iopub.execute_input":"2025-11-28T04:40:12.28522Z","iopub.status.idle":"2025-11-28T04:40:12.298829Z","shell.execute_reply.started":"2025-11-28T04:40:12.285185Z","shell.execute_reply":"2025-11-28T04:40:12.298017Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shap","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T04:41:39.271422Z","iopub.execute_input":"2025-11-28T04:41:39.27174Z","iopub.status.idle":"2025-11-28T04:41:48.452126Z","shell.execute_reply.started":"2025-11-28T04:41:39.271719Z","shell.execute_reply":"2025-11-28T04:41:48.451546Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"background = [\n    X_bg_c2v.astype(np.int32),\n    X_bg_w2v.astype(np.float32)\n]\n\ntestset = [\n    X_test_c2v.astype(np.int32),\n    X_test_w2v.astype(np.float32)\n]\n\n# ---- WRAPPER FUNCTION CHO MODEL ----\ndef f(inputs):\n    # inputs = [batch_char, batch_word]\n    return model(inputs, training=False)\n\nprint(\"Táº¡o DeepExplainer...\")\nexplainer = shap.DeepExplainer(f, background)\n\nprint(\"TÃ­nh SHAP...\")\nshap_values = explainer.shap_values(testset)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T04:44:17.290042Z","iopub.execute_input":"2025-11-28T04:44:17.290398Z","iopub.status.idle":"2025-11-28T04:44:17.318761Z","shell.execute_reply.started":"2025-11-28T04:44:17.29037Z","shell.execute_reply":"2025-11-28T04:44:17.31784Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Type of shap_values:\", type(shap_values))\n\n# Normalize output: handle cases\nif isinstance(shap_values, list) and len(shap_values) == 2:\n    char_shap = np.array(shap_values[0])\n    word_shap = np.array(shap_values[1])\nelse:\n    # If shap_values is single np.array with shape (2, m, ... ) or (m, D)\n    sv = np.array(shap_values)\n    if sv.ndim == 3 and sv.shape[0] == 1:\n        # shape (1, m, D_concat) -> take sv[0]\n        sv = sv[0]\n    if sv.ndim == 2 and sv.shape[1] == (X_c2v.shape[1] + X_w2v.shape[1]):\n        # split\n        char_shap = sv[:, :X_c2v.shape[1]]\n        word_shap = sv[:, X_c2v.shape[1]:]\n    else:\n        raise RuntimeError(\"Unexpected shap_values shape: \" + str(sv.shape))\n\nprint(\"char_shap shape:\", char_shap.shape)  # expect (n_test, 200)\nprint(\"word_shap shape:\", word_shap.shape)  # expect (n_test, 50)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature_names_word = [f\"w_{i}\" for i in range(word_dim)]\nshap.summary_plot(word_shap, X_test_w2v, feature_names=feature_names_word, show=True)\n\n# For char sequence (200 positions) - positions can be named pos_0..pos_199\nfeature_names_char = [f\"pos_{i}\" for i in range(X_c2v.shape[1])]\n# But note: these positions are token ids. To map token id -> char you can use tokenizer.index_word (if available)\nshap.summary_plot(char_shap, X_test_c2v, feature_names=feature_names_char, show=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}